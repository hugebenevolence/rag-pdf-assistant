# run_demo.py

"""
Demo script Ä‘á»ƒ test RAG Chatbot Pro
"""

import sys
import os

# Add current directory to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from modules.pdf_processor import PDFProcessor
from modules.vector_store import VectorStore
from modules.llm_wrapper import LLMWrapper
from modules.rag_pipeline import RAGPipeline
from utils.logger import get_logger
from utils.metrics import get_metrics
from config import app_config

def main():
    """Demo chÃ­nh"""
    logger = get_logger()
    metrics = get_metrics()
    
    logger.info("ğŸš€ Báº¯t Ä‘áº§u demo RAG Chatbot Pro")
    
    # ÄÆ°á»ng dáº«n tá»›i file PDF demo
    pdf_path = ""
    
    if not os.path.exists(pdf_path):
        logger.error(f"KhÃ´ng tÃ¬m tháº¥y file PDF: {pdf_path}")
        return
    
    try:
        # 1. Xá»­ lÃ½ PDF
        logger.info("ğŸ“š Äang xá»­ lÃ½ tÃ i liá»‡u PDF...")
        processor = PDFProcessor()
        chunks = processor.load_and_chunk(pdf_path)
        logger.success(f"ÄÃ£ táº¡o {len(chunks)} chunks tá»« tÃ i liá»‡u")
        
        # 2. Táº¡o vector store
        logger.info("ğŸ” Äang xÃ¢y dá»±ng vector store...")
        vector_store = VectorStore(processor.embedding_model)
        retriever = vector_store.build_store(chunks)
        logger.success("Vector store Ä‘Ã£ sáºµn sÃ ng")
        
        # 3. Khá»Ÿi táº¡o LLM
        logger.info("ğŸ¤– Äang khá»Ÿi táº¡o LLM...")
        llm = LLMWrapper().get_llm()
        logger.success("LLM Ä‘Ã£ sáºµn sÃ ng")
        
        # 4. Táº¡o RAG pipeline
        logger.info("ğŸ”— Äang táº¡o RAG pipeline...")
        pipeline = RAGPipeline(retriever, llm)
        logger.success("RAG pipeline Ä‘Ã£ sáºµn sÃ ng")
        
        # 5. Demo questions
        demo_questions = [
            "TÃ i liá»‡u nÃ y nÃ³i vá» chá»§ Ä‘á» gÃ¬?",
            "Má»¥c tiÃªu cá»§a dá»± Ã¡n lÃ  gÃ¬?",
            "CÃ³ nhá»¯ng yÃªu cáº§u ká»¹ thuáº­t nÃ o?",
            "Cáº§n lÃ m gÃ¬ Ä‘á»ƒ hoÃ n thÃ nh dá»± Ã¡n?",
            "TÃ³m táº¯t nhá»¯ng Ä‘iá»ƒm quan trá»ng nháº¥t"
        ]
        
        print("\n" + "="*80)
        print("ğŸ¯ DEMO RAG CHATBOT PRO")
        print("="*80)
        
        for i, question in enumerate(demo_questions, 1):
            print(f"\nğŸ”¸ CÃ¢u há»i {i}: {question}")
            print("-" * 60)
            
            # Tráº£ lá»i vá»›i streaming
            print("ğŸ¤– Tráº£ lá»i: ", end="", flush=True)
            for chunk in pipeline.ask_streaming(question, use_memory=True):
                print(chunk, end="", flush=True)
            print("\n")
            
            # Nháº¥n Enter Ä‘á»ƒ tiáº¿p tá»¥c
            if i < len(demo_questions):
                input("Nháº¥n Enter Ä‘á»ƒ tiáº¿p tá»¥c...")
        
        # 6. Hiá»ƒn thá»‹ metrics
        print("\n" + "="*80)
        print("ğŸ“Š METRICS & STATISTICS")
        print("="*80)
        
        current_metrics = metrics.get_metrics()
        pipeline_info = pipeline.get_pipeline_info()
        
        print(f"ğŸ“ˆ Tá»•ng sá»‘ cÃ¢u há»i: {current_metrics['questions_asked']}")
        print(f"ğŸ“„ TÃ i liá»‡u Ä‘Ã£ xá»­ lÃ½: {current_metrics['documents_processed']}")
        print(f"ğŸ§© Tá»•ng sá»‘ chunks: {current_metrics['total_chunks']}")
        print(f"â±ï¸ Thá»i gian pháº£n há»“i trung bÃ¬nh: {current_metrics['average_response_time']:.2f}s")
        print(f"âŒ Sá»‘ lá»—i: {current_metrics['errors']}")
        print(f"ğŸ§  Memory size: {pipeline_info['memory_size']}")
        print(f"ğŸ”§ Retriever type: {pipeline_info['retriever_type']}")
        
        # 7. Chat tÆ°Æ¡ng tÃ¡c
        print("\n" + "="*80)
        print("ğŸ’¬ CHAT TÆ¯Æ NG TÃC")
        print("="*80)
        print("Nháº­p cÃ¢u há»i (hoáº·c 'quit' Ä‘á»ƒ thoÃ¡t):")
        
        while True:
            try:
                user_input = input("\nğŸ§‘ Báº¡n: ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'q']:
                    break
                
                if not user_input:
                    continue
                
                print("ğŸ¤– AI: ", end="", flush=True)
                for chunk in pipeline.ask_streaming(user_input, use_memory=True):
                    print(chunk, end="", flush=True)
                print()
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ Táº¡m biá»‡t!")
                break
        
        # 8. Xuáº¥t bÃ¡o cÃ¡o
        print("\n" + "="*80)
        print("ğŸ“Š XUáº¤T BÃO CÃO")
        print("="*80)
        
        final_metrics = metrics.get_metrics()
        conversation_history = pipeline.get_conversation_history()
        
        report = {
            "session_summary": {
                "questions_asked": final_metrics['questions_asked'],
                "documents_processed": final_metrics['documents_processed'],
                "total_chunks": final_metrics['total_chunks'],
                "average_response_time": final_metrics['average_response_time'],
                "errors": final_metrics['errors']
            },
            "conversation_history": conversation_history,
            "pipeline_info": pipeline_info
        }
        
        # LÆ°u bÃ¡o cÃ¡o
        import json
        from datetime import datetime
        
        report_filename = f"demo_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ BÃ¡o cÃ¡o Ä‘Ã£ Ä‘Æ°á»£c lÆ°u vÃ o: {report_filename}")
        
        logger.success("ğŸ‰ Demo hoÃ n thÃ nh!")
        
    except Exception as e:
        logger.error(f"âŒ Lá»—i trong demo: {str(e)}")
        raise

if __name__ == "__main__":
    main()
